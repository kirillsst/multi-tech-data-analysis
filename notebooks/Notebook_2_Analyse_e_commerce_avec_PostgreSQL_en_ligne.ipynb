{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine, text\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import csv "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ytLvCF3fQxRJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aws-0-eu-west-3.pooler.supabase.com\n",
            "postgres.qlqreakdzlwedtorhxss\n",
            "Connection successful!\n",
            "Current Time: ('PostgreSQL 17.4 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 13.2.0, 64-bit',)\n",
            "Connection closed.\n"
          ]
        }
      ],
      "source": [
        "# Configuration de connexion (√† adapter selon votre provider)\n",
        "load_dotenv()\n",
        "\n",
        "# Fetch variables\n",
        "USER = os.getenv(\"USERK\")\n",
        "PASSWORD = os.getenv(\"PASS\")\n",
        "HOST = os.getenv(\"HOST\")\n",
        "PORT = os.getenv(\"PORT\")\n",
        "DBNAME = os.getenv(\"DBNAME\")\n",
        "print(HOST)\n",
        "print(USER)\n",
        "\n",
        "DATABASE_CONFIG = {\n",
        "    'host': HOST,  # Ou votre host Supabase\n",
        "    'database': DBNAME,\n",
        "    'user': USER,\n",
        "    'password': PASSWORD,\n",
        "    'port': PORT\n",
        "}\n",
        "    \n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{USER}:{PASSWORD}@\"\n",
        "    f\"{HOST}:{PORT}/{DBNAME}\"\n",
        ")\n",
        "\n",
        "# Test de connexion\n",
        "def test_connection():\n",
        "    \"\"\"\n",
        "    Testez votre connexion √† la base\n",
        "\n",
        "    √âtapes :\n",
        "    1. Utilisez pd.read_sql() pour ex√©cuter \"SELECT version()\"\n",
        "    2. Affichez la version PostgreSQL\n",
        "    3. G√©rez les erreurs de connexion\n",
        "    \"\"\"\n",
        "    # Connect to the database\n",
        "    try:\n",
        "        connection = psycopg2.connect(\n",
        "            user=USER,\n",
        "            password=PASSWORD,\n",
        "            host=HOST,\n",
        "            port=PORT,\n",
        "            dbname=DBNAME\n",
        "        )\n",
        "        print(\"Connection successful!\")\n",
        "        \n",
        "        # Create a cursor to execute SQL queries\n",
        "        cursor = connection.cursor()\n",
        "        \n",
        "        # Example query\n",
        "        cursor.execute(\"SELECT version();\")\n",
        "        result = cursor.fetchone()\n",
        "        print(\"Current Time:\", result)\n",
        "\n",
        "        # Close the cursor and connection\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        print(\"Connection closed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to connect: {e}\")\n",
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2_uVipWkQ_W8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99441 entries, 0 to 99440\n",
            "Data columns (total 5 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   customer_id               99441 non-null  object\n",
            " 1   customer_unique_id        99441 non-null  object\n",
            " 2   customer_zip_code_prefix  99441 non-null  int64 \n",
            " 3   customer_city             99441 non-null  object\n",
            " 4   customer_state            99441 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 3.8+ MB\n",
            "None customer_id                 0\n",
            "customer_unique_id          0\n",
            "customer_zip_code_prefix    0\n",
            "customer_city               0\n",
            "customer_state              0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "### üì• Import des donn√©es via API\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "def download_olist_dataset():\n",
        "    \"\"\"\n",
        "    T√©l√©charge le dataset Olist depuis Kaggle API\n",
        "\n",
        "    Alternative : Utilisez l'API publique de l'IBGE (Institut br√©silien)\n",
        "    pour des donn√©es e-commerce synth√©tiques mais r√©alistes\n",
        "    \"\"\"\n",
        "\n",
        "    # URL des donn√©es publiques br√©siliennes\n",
        "    IBGE_API = \"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\"\n",
        "\n",
        "    # R√©cup√©ration des donn√©es de villes (pour la g√©olocalisation)\n",
        "    cities_url = f\"{IBGE_API}localidades/municipios\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(cities_url)\n",
        "        cities_data = response.json()\n",
        "\n",
        "        # Convertir en DataFrame\n",
        "        cities_df = pd.DataFrame(cities_data)\n",
        "\n",
        "        # Votre code pour nettoyer et structurer\n",
        "        # Cr√©ez des donn√©es e-commerce r√©alistes bas√©es sur ces villes\n",
        "        return cities_df\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur API IBGE : {e}\")\n",
        "        return None\n",
        "# G√©n√©ration de donn√©es e-commerce r√©alistes\n",
        "def generate_ecommerce_data(cities_df, n_customers=10000):\n",
        "    \"\"\"\n",
        "    G√©n√®re des donn√©es e-commerce r√©alistes\n",
        "\n",
        "    √âtapes guid√©es :\n",
        "    1. S√©lectionnez 50 villes br√©siliennes al√©atoirement\n",
        "    2. Cr√©ez des clients avec distribution r√©aliste\n",
        "    3. G√©n√©rez des commandes avec saisonnalit√©\n",
        "    4. Ajoutez des produits avec cat√©gories coh√©rentes\n",
        "    5. Calculez des prix et frais de port bas√©s sur la distance\n",
        "    \"\"\"\n",
        "    pass\n",
        "df_customers = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_customers_dataset.csv\")\n",
        "df_orders = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_orders_dataset.csv\")\n",
        "df_products = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_products_dataset.csv\")\n",
        "df_sellers = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_sellers_dataset.csv\")\n",
        "df_order_items = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_order_items_dataset.csv\")\n",
        "#function for check NaN values\n",
        "def check_nan(data):\n",
        "    print(data.info(),data.head(10).isna().sum())\n",
        "check_nan(df_customers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "GqGIXooNSTjp"
      },
      "outputs": [],
      "source": [
        "### üóÉÔ∏è Cr√©ation des tables SQL\n",
        "def create_tables():\n",
        "    \"\"\"\n",
        "    Cr√©ez les tables dans PostgreSQL\n",
        "\n",
        "    Tips :\n",
        "    - Utilisez des SERIAL pour les IDs auto-increment\n",
        "    - Ajoutez des index sur les cl√©s √©trang√®res\n",
        "    - Incluez des contraintes de validation\n",
        "    \"\"\"\n",
        "    create_customers = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS customers (\n",
        "        customer_id VARCHAR(36) PRIMARY KEY,\n",
        "        customer_city VARCHAR(100),\n",
        "        customer_state VARCHAR(2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_orders = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS customer_orders(\n",
        "        order_id VARCHAR(36) PRIMARY KEY,\n",
        "        customer_id VARCHAR(36) REFERENCES customers(customer_id),\n",
        "        order_status VARCHAR(50),\n",
        "        order_purchase_timestamp TIMESTAMP,\n",
        "        order_delivered_customer_date TIMESTAMP\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_products = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS products(\n",
        "        product_id VARCHAR(36) PRIMARY KEY,\n",
        "        product_category_name VARCHAR(50),\n",
        "        product_weight_g FLOAT\n",
        "    )\n",
        "    \"\"\"\n",
        "    create_sellers = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS sellers(\n",
        "    seller_id VARCHAR(36) PRIMARY KEY,\n",
        "    seller_city VARCHAR(50),\n",
        "    seller_state CHAR(2)\n",
        "    )\n",
        "    \"\"\"\n",
        "    create_order_items = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items(\n",
        "        order_id VARCHAR(36) REFERENCES customer_orders(order_id),\n",
        "        product_id VARCHAR(36) REFERENCES products(product_id),\n",
        "        seller_id VARCHAR(36) REFERENCES sellers(seller_id),\n",
        "        price NUMERIC(10, 2),\n",
        "        freight_value NUMERIC(10, 2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    # Compl√©tez pour les autres tables\n",
        "    # N'oubliez pas les contraintes de cl√©s √©trang√®res !\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        # conn.execute(text(create_customers))\n",
        "        # conn.execute(text(create_orders))\n",
        "        # conn.execute(text(create_products))\n",
        "        # conn.execute(text(create_sellers))\n",
        "        conn.execute(text(create_order_items))\n",
        "        # Ex√©cutez les autres CREATE TABLE\n",
        "        conn.commit()\n",
        "create_tables()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Je charge la data et nettoyage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_customers = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_customers_dataset.csv\")\n",
        "df_customers = df_customers.drop(columns=[\"customer_unique_id\",\"customer_zip_code_prefix\"])\n",
        "df_orders = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_orders_dataset.csv\")\n",
        "df_orders = df_orders.drop(columns=['order_approved_at', 'order_delivered_carrier_date', 'order_estimated_delivery_date'])\n",
        "df_products = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_products_dataset.csv\")\n",
        "df_products = df_products.drop(columns=[\"product_length_cm\",\"product_height_cm\",\"product_width_cm\"])\n",
        "df_products = df_products.drop(columns=['product_name_lenght', 'product_description_lenght', 'product_photos_qty'])\n",
        "df_sellers = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_sellers_dataset.csv\")\n",
        "df_sellers = df_sellers.drop(columns=['seller_zip_code_prefix'])\n",
        "df_order_items = pd.read_csv(\"~/multi-tech-data-analysis/archive/olist_order_items_dataset.csv\")\n",
        "df_order_items = df_order_items.drop(columns=['order_item_id','shipping_limit_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def df_to_sql(data_csv, name_table=str):\n",
        "    data_csv.to_sql(name_table, engine.connect(), if_exists='append', index=False)\n",
        "df_to_sql(df_order_items, name_table=\"order_items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "-- Cr√©ez les segments RFM (Champions, Loyaux, √Ä risque, etc.)\n",
        "SELECT\n",
        "    customer_id,\n",
        "    customer_state,\n",
        "    recency_score,\n",
        "    frequency_score,\n",
        "    monetary_score,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        -- Ajoutez les autres segments\n",
        "        ELSE 'Others'\n",
        "    END as customer_segment\n",
        "FROM customer_metrics;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
